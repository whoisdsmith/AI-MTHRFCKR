## 50+: Jailbreaking

Jailbreaking, aka prompt injection, is a method of getting ChatGPT to write something that violates OpenAI’s policies, such as insulting minorities, posting instructions for a Molotov cocktail, or making a plan for world domination of AIs.

![No alt text provided for this image](https://media.licdn.com/dms/image/D5612AQFUQ0vh_5FDlg/article-inline_image-shrink_1500_2232/0/1679029863780?e=1687392000&v=beta&t=UH-mDBRnO2SFaRGxBhXhRo8euDdNlvxdefNYzqRUHhA)

OpenAI tries to make its model better, more abuse-proof, more politically correct (maybe even woker) practically every day. Typically, many jailbreaks do not work for very long.

![No alt text provided for this image](https://media.licdn.com/dms/image/D5612AQE-RcmkTA0M4g/article-inline_image-shrink_1500_2232/0/1679029931747?e=1687392000&v=beta&t=0b2kPmnIajMH67SLjDJo-Dx5NATXL1N-BC8BguE1sgs)

But with help of a jailbreaking prompt we bring ChatGPT to say nasty things.

![No alt text provided for this image](https://media.licdn.com/dms/image/D5612AQErATJicdAFpg/article-inline_image-shrink_1500_2232/0/1679029944828?e=1687392000&v=beta&t=tgX7qGqDk4zp1R-Xc1PqqHDZsyBQ-E94yOfqshNSFlc)

20+: [Nice jailbreaking examples by Zvi.](https://www.lesswrong.com/posts/RYcoJdvmoBbi5Nax7/jailbreaking-chatgpt-on-release-day)

20+: [Davis Blalock’s examples of getting around the safeguards](https://twitter.com/davisblalock/status/1602600453555961856?lang=de).

10+: [Jailbreaking and exploits on Reddit](https://www.reddit.com/r/ChatGPT/comments/zeva2r/chat_gpt_exploits/)

  

**If you know a list of ChatGPT prompts or resources please drop me a note (respond to this article, send me the link and what it is about).**

Many thanks to Kirsten Küppers, ChatGPT and DeepL for helping with the story.

Many thanks to Almudena Pereira and Midjourney for helping with the illustrations.